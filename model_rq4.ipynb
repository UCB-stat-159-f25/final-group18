{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce454710-7423-422a-bea2-6ce6924a02d9",
   "metadata": {},
   "source": [
    "# Modeling: Predicting Food Affordability\n",
    "## RQ4: When all predictors are considered together, which variables contribute the most to predicting affordability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0371fbb1-5d78-46d0-876f-395fa334ac70",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "RQ4 asks which variables contribute most when predictors are considered jointly. We evaluate two feature sets:\n",
    "\n",
    "* **Full:** `median_income + cost_yr + {region_name, geotype, race_eth_name}`\n",
    "* **NoCost:** `median_income + {region_name, geotype, race_eth_name}`\n",
    "\n",
    "We fit several model families using a fixed train/test split and select the best-performing model within each feature set. To quantify each variable’s contribution, we use **permutation importance** on the held-out test set.\n",
    "\n",
    "**Note:** If `cost_yr` is mechanically related to how `affordability_ratio` is constructed, it may dominate importance and inflate performance in the Full setting. The NoCost setting helps isolate how much predictive signal remains in income + contextual variables alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54bf38e-5e73-4b0c-950e-14050cea7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from utils.model_utils import rmse, make_ohe, split_cols, make_preprocessor, wrap_log1p, eval_model_rq4\n",
    "\n",
    "RANDOM_STATE = 159\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA_PATH = Path(\"./data/food_affordability.csv\")\n",
    "OUT_DIR = Path(\"./outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "FIG_DIR = Path(\"./figures\"); FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TARGET = \"affordability_ratio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d0559c-dc06-4ed2-bd34-e637dac9e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows kept: 3473\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affordability_ratio</th>\n",
       "      <th>cost_yr</th>\n",
       "      <th>geotype</th>\n",
       "      <th>median_income</th>\n",
       "      <th>race_eth_name</th>\n",
       "      <th>region_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.315779</td>\n",
       "      <td>7508.289655</td>\n",
       "      <td>CA</td>\n",
       "      <td>23777.0</td>\n",
       "      <td>AIAN</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.194980</td>\n",
       "      <td>7508.289655</td>\n",
       "      <td>CA</td>\n",
       "      <td>38508.0</td>\n",
       "      <td>Asian</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.286664</td>\n",
       "      <td>7508.289655</td>\n",
       "      <td>CA</td>\n",
       "      <td>26192.0</td>\n",
       "      <td>AfricanAm</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.328475</td>\n",
       "      <td>7508.289655</td>\n",
       "      <td>CA</td>\n",
       "      <td>22858.0</td>\n",
       "      <td>Latino</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.204379</td>\n",
       "      <td>7508.289655</td>\n",
       "      <td>CA</td>\n",
       "      <td>36737.0</td>\n",
       "      <td>NHOPI</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   affordability_ratio      cost_yr geotype  median_income race_eth_name  \\\n",
       "0             0.315779  7508.289655      CA        23777.0          AIAN   \n",
       "1             0.194980  7508.289655      CA        38508.0         Asian   \n",
       "2             0.286664  7508.289655      CA        26192.0     AfricanAm   \n",
       "3             0.328475  7508.289655      CA        22858.0        Latino   \n",
       "4             0.204379  7508.289655      CA        36737.0         NHOPI   \n",
       "\n",
       "  region_name  \n",
       "0  California  \n",
       "1  California  \n",
       "2  California  \n",
       "3  California  \n",
       "4  California  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Define feature sets\n",
    "CONTEXT_FEATURES = [\"region_name\", \"geotype\", \"race_eth_name\"]\n",
    "FULL_FEATURES    = [\"median_income\", \"cost_yr\"] + CONTEXT_FEATURES\n",
    "NOCOST_FEATURES  = [\"median_income\"] + CONTEXT_FEATURES\n",
    "\n",
    "ALL_USED = sorted(set([TARGET] + FULL_FEATURES + NOCOST_FEATURES))\n",
    "df_m = df.dropna(subset=ALL_USED).copy()\n",
    "\n",
    "print(\"Rows kept:\", df_m.shape[0])\n",
    "df_m[ALL_USED].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f77dee-44e7-45b6-9043-02127f372476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2778, 5), (695, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = df_m[sorted(set(FULL_FEATURES + NOCOST_FEATURES))].copy()\n",
    "y = df_m[TARGET].copy()\n",
    "\n",
    "idx = np.arange(len(df_m))\n",
    "idx_train, idx_test = train_test_split(idx, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "X_train_all, X_test_all = X_all.iloc[idx_train], X_all.iloc[idx_test]\n",
    "y_train, y_test = y.iloc[idx_train], y.iloc[idx_test]\n",
    "\n",
    "X_train_all.shape, X_test_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52d7e74-4a62-4193-8443-ce9d07ed3bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_set</th>\n",
       "      <th>model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RQ4-Full (income+cost+context)</td>\n",
       "      <td>RandomForest(log1p y)</td>\n",
       "      <td>0.023730</td>\n",
       "      <td>0.006445</td>\n",
       "      <td>0.996411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RQ4-Full (income+cost+context)</td>\n",
       "      <td>HistGB(log1p y)</td>\n",
       "      <td>0.027553</td>\n",
       "      <td>0.009115</td>\n",
       "      <td>0.995162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RQ4-Full (income+cost+context)</td>\n",
       "      <td>Linear(log1p y)</td>\n",
       "      <td>0.340012</td>\n",
       "      <td>0.145797</td>\n",
       "      <td>0.263215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RQ4-Full (income+cost+context)</td>\n",
       "      <td>Dummy(mean)</td>\n",
       "      <td>0.396309</td>\n",
       "      <td>0.219434</td>\n",
       "      <td>-0.000969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RQ4-NoCost (income+context)</td>\n",
       "      <td>HistGB(log1p y)</td>\n",
       "      <td>0.084735</td>\n",
       "      <td>0.037181</td>\n",
       "      <td>0.954241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RQ4-NoCost (income+context)</td>\n",
       "      <td>RandomForest(log1p y)</td>\n",
       "      <td>0.094195</td>\n",
       "      <td>0.039957</td>\n",
       "      <td>0.943453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RQ4-NoCost (income+context)</td>\n",
       "      <td>Linear(log1p y)</td>\n",
       "      <td>0.337358</td>\n",
       "      <td>0.145718</td>\n",
       "      <td>0.274672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RQ4-NoCost (income+context)</td>\n",
       "      <td>Dummy(mean)</td>\n",
       "      <td>0.396309</td>\n",
       "      <td>0.219434</td>\n",
       "      <td>-0.000969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature_set                  model      RMSE       MAE  \\\n",
       "2  RQ4-Full (income+cost+context)  RandomForest(log1p y)  0.023730  0.006445   \n",
       "3  RQ4-Full (income+cost+context)        HistGB(log1p y)  0.027553  0.009115   \n",
       "1  RQ4-Full (income+cost+context)        Linear(log1p y)  0.340012  0.145797   \n",
       "0  RQ4-Full (income+cost+context)            Dummy(mean)  0.396309  0.219434   \n",
       "7     RQ4-NoCost (income+context)        HistGB(log1p y)  0.084735  0.037181   \n",
       "6     RQ4-NoCost (income+context)  RandomForest(log1p y)  0.094195  0.039957   \n",
       "5     RQ4-NoCost (income+context)        Linear(log1p y)  0.337358  0.145718   \n",
       "4     RQ4-NoCost (income+context)            Dummy(mean)  0.396309  0.219434   \n",
       "\n",
       "         R2  \n",
       "2  0.996411  \n",
       "3  0.995162  \n",
       "1  0.263215  \n",
       "0 -0.000969  \n",
       "7  0.954241  \n",
       "6  0.943453  \n",
       "5  0.274672  \n",
       "4 -0.000969  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sets = {\n",
    "    \"RQ4-Full (income+cost+context)\": FULL_FEATURES,\n",
    "    \"RQ4-NoCost (income+context)\": NOCOST_FEATURES\n",
    "}\n",
    "\n",
    "rows = []\n",
    "best_models = {}\n",
    "all_preds = {}\n",
    "\n",
    "for fs_name, feats in feature_sets.items():\n",
    "    Xtr = X_train_all[feats]\n",
    "    Xte = X_test_all[feats]\n",
    "\n",
    "    # Baseline\n",
    "    dummy = DummyRegressor(strategy=\"mean\")\n",
    "    dummy.fit(Xtr, y_train)\n",
    "    pred_dummy = dummy.predict(Xte)\n",
    "    rows.append({\n",
    "        \"feature_set\": fs_name,\n",
    "        \"model\": \"Dummy(mean)\",\n",
    "        \"RMSE\": rmse(y_test, pred_dummy),\n",
    "        \"MAE\": float(mean_absolute_error(y_test, pred_dummy)),\n",
    "        \"R2\":  float(r2_score(y_test, pred_dummy))\n",
    "    })\n",
    "    all_preds[(fs_name, \"Dummy(mean)\")] = pred_dummy\n",
    "\n",
    "    # Linear (good baseline)\n",
    "    pre_lin = make_preprocessor(feats, X_train_all, dense=False, scale_num_for_linear=True)\n",
    "    lin = wrap_log1p(Pipeline([(\"pre\", pre_lin), (\"lin\", LinearRegression())]))\n",
    "    m_lin, pred_lin = eval_model_rq4(lin, Xtr, Xte, y_train, y_test)\n",
    "    rows.append({\"feature_set\": fs_name, \"model\": \"Linear(log1p y)\", **m_lin})\n",
    "    all_preds[(fs_name, \"Linear(log1p y)\")] = pred_lin\n",
    "\n",
    "    # Random Forest\n",
    "    pre_tree = make_preprocessor(feats, X_train_all, dense=False, scale_num_for_linear=False)\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        min_samples_leaf=2\n",
    "    )\n",
    "    rf_m = wrap_log1p(Pipeline([(\"pre\", pre_tree), (\"rf\", rf)]))\n",
    "    m_rf, pred_rf = eval_model_rq4(rf_m, Xtr, Xte, y_train, y_test)\n",
    "    rows.append({\"feature_set\": fs_name, \"model\": \"RandomForest(log1p y)\", **m_rf})\n",
    "    all_preds[(fs_name, \"RandomForest(log1p y)\")] = pred_rf\n",
    "\n",
    "    pre_hgb = make_preprocessor(feats, X_train_all, dense=True, scale_num_for_linear=False)\n",
    "    hgb = HistGradientBoostingRegressor(\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=400\n",
    "    )\n",
    "    hgb_m = wrap_log1p(Pipeline([(\"pre\", pre_hgb), (\"hgb\", hgb)]))\n",
    "    m_hgb, pred_hgb = eval_model_rq4(hgb_m, Xtr, Xte, y_train, y_test)\n",
    "    rows.append({\"feature_set\": fs_name, \"model\": \"HistGB(log1p y)\", **m_hgb})\n",
    "    all_preds[(fs_name, \"HistGB(log1p y)\")] = pred_hgb\n",
    "\n",
    "metrics_rq4 = pd.DataFrame(rows).sort_values([\"feature_set\", \"RMSE\"])\n",
    "metrics_rq4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf943d04-b853-4811-9e55-1afa63423872",
   "metadata": {},
   "source": [
    "Tree-based models strongly outperform the linear baseline. With the Full feature set, Random Forest achieves near-perfect performance (R² ≈ 0.996; RMSE ≈ 0.024), while without `cost_yr`, the best model (HistGradientBoosting) still performs very well (R² ≈ 0.954; RMSE ≈ 0.085). The linear model remains around R² ≈ 0.26–0.27, indicating that nonlinear models capture structure that linear regression does not.\n",
    "\n",
    "**Note:** Because `cost_yr` may be mechanically related to `affordability_ratio` (depending on how the affordability ratio was constructed), the extremely high R² in the Full setting should be interpreted cautiously as potentially reflecting a definitional/structural relationship rather than purely “discovering” new predictive patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1bedef5-7d2a-475d-9718-5a0117ceed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics_rq4.to_csv(OUT_DIR / \"rq4_model_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a9f9c-b6f2-4665-8dd1-2e54d1d4141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_best(df_metrics, fs_name):\n",
    "    sub = df_metrics[df_metrics[\"feature_set\"] == fs_name].copy()\n",
    "    sub = sub.sort_values(\"RMSE\")\n",
    "    return sub.iloc[0][\"model\"]\n",
    "\n",
    "for fs_name, feats in feature_sets.items():\n",
    "    best_model_name = pick_best(metrics_rq4, fs_name)\n",
    "    preds = all_preds[(fs_name, best_model_name)]\n",
    "\n",
    "    # Rebuild and refit the best model (so we can run permutation importance cleanly)\n",
    "    Xtr = X_train_all[feats]\n",
    "    Xte = X_test_all[feats]\n",
    "\n",
    "    if best_model_name.startswith(\"Linear\"):\n",
    "        pre = make_preprocessor(feats, X_train_all, dense=False, scale_num_for_linear=True)\n",
    "        best = wrap_log1p(Pipeline([(\"pre\", pre), (\"lin\", LinearRegression())]))\n",
    "    elif best_model_name.startswith(\"RandomForest\"):\n",
    "        pre = make_preprocessor(feats, X_train_all, dense=False, scale_num_for_linear=False)\n",
    "        best = wrap_log1p(Pipeline([(\"pre\", pre), (\"rf\", RandomForestRegressor(\n",
    "            n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1, min_samples_leaf=2\n",
    "        ))]))\n",
    "    elif best_model_name.startswith(\"HistGB\"):\n",
    "        pre = make_preprocessor(feats, X_train_all, dense=True, scale_num_for_linear=False)\n",
    "        best = wrap_log1p(Pipeline([(\"pre\", pre), (\"hgb\", HistGradientBoostingRegressor(\n",
    "            random_state=RANDOM_STATE, max_depth=6, learning_rate=0.05, max_iter=400\n",
    "        ))]))\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    best.fit(Xtr, y_train)\n",
    "\n",
    "    # Pred vs True\n",
    "    plt.figure()\n",
    "    plt.scatter(y_test, preds, s=10)\n",
    "    plt.xlabel(\"True affordability_ratio\")\n",
    "    plt.ylabel(\"Predicted affordability_ratio\")\n",
    "    plt.title(f\"RQ4 Pred vs True | {fs_name} | {best_model_name}\")\n",
    "    fig_path = FIG_DIR / f\"rq4_pred_vs_true__{fs_name.split()[0].lower()}__{best_model_name.split('(')[0].lower()}.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # Permutation importance on RAW features (region/geotype/race/income/cost)\n",
    "    imp = permutation_importance(\n",
    "        best,\n",
    "        Xte,\n",
    "        y_test,\n",
    "        n_repeats=20,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        scoring=\"r2\"\n",
    "    )\n",
    "\n",
    "    imp_df = pd.DataFrame({\n",
    "        \"feature\": Xte.columns,\n",
    "        \"importance_mean\": imp.importances_mean,\n",
    "        \"importance_std\": imp.importances_std,\n",
    "    }).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "    print(\"\\n=== Permutation importance:\", fs_name, \"|\", best_model_name, \"===\\n\")\n",
    "    display(imp_df)\n",
    "\n",
    "    # Save + plot\n",
    "    out_csv = OUT_DIR / f\"rq4_perm_importance__{fs_name.split()[0].lower()}__{best_model_name.split('(')[0].lower()}.csv\"\n",
    "    imp_df.to_csv(out_csv, index=False)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.barh(imp_df[\"feature\"][::-1], imp_df[\"importance_mean\"][::-1])\n",
    "    plt.xlabel(\"Permutation importance (mean drop in R²)\")\n",
    "    plt.title(f\"RQ4 Importance | {fs_name} | {best_model_name}\")\n",
    "    out_fig = FIG_DIR / f\"rq4_perm_importance__{fs_name.split()[0].lower()}__{best_model_name.split('(')[0].lower()}.png\"\n",
    "    plt.savefig(out_fig, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc64889d-6e5f-45d4-b57d-b779411b0fe0",
   "metadata": {},
   "source": [
    "Permutation importance (measured as mean drop in test R² when a feature is permuted) shows that **median_income dominates** in both settings.  \n",
    "- In the **Full** model, `median_income` is by far the largest driver, with `cost_yr` as a distant second; `region_name`, `race_eth_name`, and `geotype` contribute negligibly.  \n",
    "- In the **NoCost** model, `median_income` remains dominant; `region_name` becomes the next most informative feature, while `race_eth_name` is small and `geotype` is near zero.\n",
    "\n",
    "Overall, when all predictors are considered together, **income is the primary driver**, and **cost (when included) adds additional predictive power**, while geographic/race context variables contribute comparatively little in this dataset/modeling setup.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
